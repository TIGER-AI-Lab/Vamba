<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="We propose Vamba, a hybrid Mamba-Transformer model that leverages cross-attention layers and Mamba-2 blocks for efficient hour-long video understanding.">
    <meta property="og:title" content="Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers" />
    <meta property="og:description" content="We propose Vamba, a hybrid Mamba-Transformer model that leverages cross-attention layers and Mamba-2 blocks for efficient hour-long video understanding." />
    <meta property="og:url" content="https://tiger-ai-lab.github.io/Vamba/" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/images/vamba_teaser.png" />
    <meta property="og:image:width" content="800" />
    <meta property="og:image:height" content="600" />


    <meta name="twitter:title" content="Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers">
    <meta name="twitter:description" content="We propose Vamba, a hybrid Mamba-Transformer model that leverages cross-attention layers and Mamba-2 blocks for efficient hour-long video understanding.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/vamba_teaser.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="vamba">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Vamba</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¦‰</text></svg>"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="static/js/jquery.min.js"></script>
    <script src="static/js/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>

    <link rel="stylesheet" type="text/css" href="static/css/jquery.dataTables.css">
    <script type="text/javascript" charset="utf8" src="static/js/jquery-3.5.1.js"></script>
    <script type="text/javascript" charset="utf8" src="static/js/jquery.dataTables.js"></script>
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                          Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                                <a href="https://cs.uwaterloo.ca/~w2ren" target="_blank">Weiming Ren</a><sup>1,4</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://iamtonymwt.github.io/" target="_blank">Wentao Ma</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://hyang0511.github.io/" target="_blank">Huan Yang</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://congwei1230.github.io/" target="_blank">Cong Wei</a><sup>1,4</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?hl=en&user=qyTrq4kAAAAJ" target="_blank">Ge Zhang</a><sup>1,5</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a><sup>1,2</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                            <sup>1</sup>University of Waterloo,
                            <sup>2</sup>University of Toronto,
                            <sup>3</sup>01.AI,
                            <sup>4</sup>Vector Institute,
                            <sup>5</sup>M-A-P
                            </span>
                            <span class="author-block">
                                <small>
                                    w2ren@uwaterloo.ca,
                                    wenhuchen@uwaterloo.ca
                                </small>
                            </span>

                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2503.11579" target="_blank"
                                      class="external-link button is-normal is-rounded is-dark">
                                          <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                          </span>
                                    <span>arXiv</span>
                                    </a>
                                    </span>

                                <span class="link-block">
                                    <a href="https://huggingface.co/TIGER-Lab/Vamba" target="_blank"
                                    class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                          ðŸ¤—
                                        </span>
                                        <span>Model</span>
                                    </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                                  <a href="https://github.com/TIGER-AI-Lab/Vamba" target="_blank"
                                  class="external-link button is-normal is-rounded is-dark">
                                  <span class="icon">
                                    <i class="fab fa-github"></i>
                                  </span>
                                      <span>Code</span>
                                  </a>
                                </span>
                                
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h1 class="title is-2">Abstract</h1>
            <div class="content has-text-justified">
                            <p>State-of-the-art transformer-based large multimodal models (LMMs) struggle to handle hour-long video inputs due to the quadratic complexity of the causal self-attention operations, leading to high computational costs during training and inference. Existing token compression-based methods reduce the number of video tokens but often incur information loss and remain inefficient for extremely long sequences. In this paper, we explore an orthogonal direction to build a hybrid Mamba-Transformer model (VAMBA) that employs Mamba-2 blocks to encode video tokens with linear complexity. Without any token reduction, VAMBA can encode more than 1024 frames (640Ã—360) on a single GPU, while transformer-based models can only encode 256 frames. On long video input, VAMBA achieves at least 50% reduction in GPU memory usage during training and inference, and nearly doubles the speed per training step compared to transformer-based LMMs. Our experimental results demonstrate that VAMBA improves accuracy by 4.6% on the challenging hour-long video understanding benchmark LVBench over prior efficient video LMMs, and maintains strong performance on a broad spectrum of long and short video understanding tasks.</p>
            </div>
          </div>
        </div>
      </div>
    </section>



    <!-- Image carousel -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/vamba_teaser.png" alt="VISTA" style="width: 70%; display: block; margin-left: auto; margin-right: auto;"/>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End image carousel -->


    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h1 class="title is-1">Vamba</h1>
            <div class="content has-text-justified">
              <p>
                To reduce the computational/memory cost for video LMMs, previous efforts have primarily focused on reducing the vision tokens in the input sequence. In this study, we investigate an orthogonal direction to previous approaches: instead of compressing the video tokens, we seek to develop an alternative model architecture that improves the efficiency of processing video tokens during training and pre-filling stage of inference. We propose Vamba, a hybrid Mamba-Transformer model for efficient hour-long video understanding. The key insight of our method is that we can design efficient modules to approximate the causal self-attention operation for both text and video tokens in transformer-based LMMs. In particular, we propose to (1) utilize cross-attentions to update text tokens based on video tokens, which is affordable due to the short length of text tokens, and (2) adopt Mamba-2 to process the massive video tokens with linear complexity.
              </p>
              <p>
                The main computation overhead in the transformer-based LMMs comes from the <b>quadratic complexity of the self-attention in the video tokens</b>. To overcome this issue, we design a hybrid Mamba Transformer architecture to process text and video tokens differently. The key idea of our method is to split the expensive self-attention operation over the entire video and text token sequence into two more efficient components. Since video tokens typically dominate the sequence while text tokens remain few, we maintain the self-attention mechanism exclusively for the text tokens and eliminate it for the video tokens. Instead, we add cross-attention layers that use text tokens as queries and video tokens as keys and values. In the meantime, we propose to employ Mamba blocks to effectively process the video tokens.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>


        <!-- Image carousel -->
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/vamba_main.png" alt="VISTA" />
<!--                            <h2 class="subtitle">-->
<!--                            </h2>-->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End image carousel -->


    <section class="section hero is-light">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-full">
              <h1 class="title is-1">Runtime Efficiency</h1>
                <div class="content has-text-justified">
                <p>
                  To understand our modelâ€™s runtime efficiency gains over the baseline transformer-based LMM (Qwen2-VL-7B), we conduct an efficiency analysis for both training and inference. Our results show that our model requires over 50% less training memory when processing videos with more than 16 frames. This efficiency gain allows us to handle a larger number of video frames during training (512 vs. 128). Furthermore, our efficient design also accelerates model training, achieving nearly twice the speedup per training step when working with more than 64 frames. For model inference, Vamba's memory usage increases more slowly as the frame increases, allowing it to handle four times as many frames on a single NVIDIA A800 80G GPU compared to Qwen2-VL-7B (256 vs. 1024). Regarding computational cost, Vamba reduces FLOPs by 30% to 50% during inference, demonstrating a significantly lower complexity than its transformer-based LMM counterparts.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/vamba_efficiency.png" alt="HRVideoBench" style="width: 100%; display: block; margin-left: auto; margin-right: auto;"/>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <h1 class="title is-1">Experimental Results</h1>
              <div class="content has-text-justified">
              <p>
                We conduct extensive evaluations across various video understanding tasks to demonstrate the effectiveness of Vamba.
              </p>
              <p>
                <b>Hour-long Video Understanding:</b> Vamba consistently outperforms all efficient video LMMs across the three hour-long video benchmarks, highlighting its exceptional ability to understand and reason over hour-scale videos. Notably, our model surpasses the baseline Qwen2-VL-7B on the LVBench benchmark, and its performance on HourVideo is also very close to Qwen2-VL-7B. These results underscore that Vamba is competitive with the best open-sourced transformer-based LMMs, while being significantly more efficient during training and inference.
              </p>
              <p>
                <b>Medium-Length or Short Video Understanding:</b> Vamba demonstrates superior performance across three medium-length video understanding benchmarks (with average video durations between 10â€“20 minutes), ranking first among efficient video LMMs on all metrics. Vamba also achieves competitive performances, ranking first on NExT-QA and DREAM-1K and second on MVBench among efficient LMMs. Overall, our model delivers the best results on medium-length and long video benchmarks, demonstrating its strong ability to handle long-context video-language inputs.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered is-fifths-fifths">
                        <!-- <h2 class="title is-3">Top 5 Candidates Retrieved from M-BEIR</h2> -->
                        <div class="content has-text-justified">
                            <div class="buttonGroup"  data-target-display="#imageDisplayArea2" style="margin-bottom: 2em;">
                                <button value="hour_long" data-img-path="static/images/hour_long.png" >Hour-long Video Understanding Results</button>
                                <button value="others" data-img-path="static/images/others.png">Medium-Length or Short Video Understanding Results</button>
                                <button value="case" data-img-path="static/images/case.png">Case Study</button>
                            </div>

                            <div id="imageDisplayArea2">
                                <img id="displayedImage2" src="static/images/hour_long.png" alt="hour_long" />
                            </div>
                    </div>
                </div>
            </div>
        </div>
        </div>
    </section>


    <!-- BibTex citation -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Reference</h2>
            Please kindly cite our paper if you use our code, data, models or results:
            <br><br>
            <pre><code>@misc{ren2025vambaunderstandinghourlongvideos,
      title={Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers}, 
      author={Weiming Ren and Wentao Ma and Huan Yang and Cong Wei and Ge Zhang and Wenhu Chen},
      year={2025},
      eprint={2503.11579},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.11579}, 
}</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">

                        <p>
                            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                                target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>

                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>
<style>
    .buttonGroup {
        text-align: center;
    }
    
    .buttonGroup>button {
        padding: 15px;
        color: white;
        background-color: #363636;
        border-radius: 5px;
    }
    
    .buttonGroup>button:hover {
        box-shadow: 5px;
    }
</style>

</html>
